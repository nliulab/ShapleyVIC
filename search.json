[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ShapleyVIC: Shapley Variable Importance Cloud for Interpretable Machine Learning",
    "section": "",
    "text": "ShapleyVIC Introduction\nVariable importance assessment is important for interpreting machine learning models. Current practice in interpretable machine learning applications focuses on explaining the final models that optimize predictive performance. However, this does not fully address practical needs, where researchers are willing to consider models that are “good enough” but are easier to understand or implement. Shapley variable importance cloud (ShapleyVIC) fills this gap by extending current method to a set of “good models” for comprehensive and robust assessments. Building on a common theoretical basis (i.e., Shapley values for variable importance), ShapleyVIC seamlessly complements the widely adopted SHAP assessments of a single final model to avoid biased inference. Please visit GitHub page for source code."
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "ShapleyVIC: Shapley Variable Importance Cloud for Interpretable Machine Learning",
    "section": "Usage",
    "text": "Usage\nAs detailed in Chapter 3 ShapleyVIC analysis of variable importance consists of 3 general steps:\n\nTraining an optimal prediction model (e.g., a logistic regression model).\nGenerating a reasonable number of (e.g., 350) nearly optimal models of the same model class (e.g., logistic regression).\nEvaluate Shapley-based variable importance from each nearly optimal model and pool information for inference.\n\nShapleyVIC does not require variable centering or standardization, but requires some data checking and pre-processing for stable and smooth processing, which we summarize in Chapter 2.\nThe ShapleyVIC-based variable ranking can also be used with the AutoScore framework to develop clinical risk scores for interpretable risk prediction, which we demonstrate in Chapter 4 and Chapter 5."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "ShapleyVIC: Shapley Variable Importance Cloud for Interpretable Machine Learning",
    "section": "Installation",
    "text": "Installation\nThe ShapleyVIC framework is now implemented using a Python library that trains the optimal model, generates nearly optimal models and evaluate Shapley-based variable importance from such models, and an R package that pools information across models to generate summary statistics and visualizations for inference.\n\nPython library\n\nRequired: Python version 3.6 or higher.\n\nRecommended: latest stable release of Python 3.9 or 3.10.\n\nRequired: latest version of git.\n\nExecute the following command in Terminal/Command Prompt to install the Python library from GitHub:\n\n# Linux/macOS: \npip install git+\"https://github.com/nyilin/ShapleyVIC-Python#egg=ShapleyVIC&subdirectory=python\"\n\n# Windows:\npython.exe -m pip install git+\"https://github.com/nyilin/ShapleyVIC-Python#egg=ShapleyVIC&subdirectory=python\"\n\n\n\n\n\n\n\nNote\n\n\n\n\nShapleyVIC uses a modified version of the SAGE library (version 0.0.4b1), which avoids occasional stack overflow problems on Windows but does not affect variable importance evaluation.\n\n\n\n\n\nR package\n\nRequired: R version 3.5.0 or higher.\n\nRecommended: use latest version of R with RStudio.\n\n\nExecute the following command in R/RStudio to install the R package from GitHub:\n\nif (!require(\"devtools\", quietly = TRUE)) install.packages(\"devtools\")\ndevtools::install_github(\"nliulab/ShapleyVIC/r\")"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "ShapleyVIC: Shapley Variable Importance Cloud for Interpretable Machine Learning",
    "section": "Citation",
    "text": "Citation\n\nCore paper\n\nNing Y, Ong ME, Chakraborty B, Goldstein BA, Ting DS, Vaughan R, Liu N. Shapley variable importance cloud for interpretable machine learning. Patterns 2022\n\n\n\nMethod extension\n\nNing Y, Li S, Ong ME, Xie F, Chakraborty B, Ting DS, Liu N. A novel interpretable machine learning system to generate clinical risk scores: An application for predicting early mortality or unplanned readmission in a retrospective cohort study. PLOS Digit Health 1(6): e0000062."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "ShapleyVIC: Shapley Variable Importance Cloud for Interpretable Machine Learning",
    "section": "Contact",
    "text": "Contact\n\nYilin Ning (Email: yilin.ning@duke-nus.edu.sg)\nNan Liu (Email: liu.nan@duke-nus.edu.sg)"
  },
  {
    "objectID": "data.html#general-requirements",
    "href": "data.html#general-requirements",
    "title": "1  Data requirements",
    "section": "1.1 General requirements",
    "text": "1.1 General requirements\n\nCurrently ShapleyVIC only applies to binary outcomes.\nCode binary outcomes as 0/1.\nNo space or special characters (e.g., [, ], (, ), ,) in variable names. Replace them using _.\nVariable centering/standardization is not required."
  },
  {
    "objectID": "data.html#missing-values-and-sparsity",
    "href": "data.html#missing-values-and-sparsity",
    "title": "1  Data requirements",
    "section": "1.2 Missing values and sparsity",
    "text": "1.2 Missing values and sparsity\n\nHandle missing entries appropriately before applying ShapleyVIC. Missing entry is not allowed.\nCheck data distribution and handle data sparsity before applying ShapleyVIC. Data sparsity may increase run time and lead to unstable results."
  },
  {
    "objectID": "data.html#additional-pre-processing-for-high-dimensional-data",
    "href": "data.html#additional-pre-processing-for-high-dimensional-data",
    "title": "1  Data requirements",
    "section": "1.3 Additional pre-processing for high-dimensional data",
    "text": "1.3 Additional pre-processing for high-dimensional data\n\nAlthough theoretically permissible, it is not advisable to apply ShapleyVIC to data with a large number of variables.\nScreen out variables with low importance (e.g., based on univariable or multivariable analysis p-values) to reduce dimension (e.g., to <50 variables) before applying ShapleyVIC."
  },
  {
    "objectID": "shapleyvic.html#python-shapleyvic-calculation",
    "href": "shapleyvic.html#python-shapleyvic-calculation",
    "title": "2  ShapleyVIC for Variable Importance Assessment",
    "section": "2.1 [Python] ShapleyVIC calculation",
    "text": "2.1 [Python] ShapleyVIC calculation\nThis part of the ShapleyVIC workflow is implemented in Python.\nIn this part of the workflow, we load and prepare data, train optimal logistic regression model, generate nearly optimal models, and compute Shapley-based variable importance for each model.\n\n2.1.1 Load data\n\nRead data from CSV or Excel files.\nFor this demo, use the integrated data in the library that contains 7214 samples analyzed in Experiment 1 (i.e., the recidivism prediction study) of the paper.\n\n\nfrom ShapleyVIC import df_compas\n\ncompas = df_compas.load_data()\n# See data description using the following command:\n# help(df_compas.load_data)\ncompas.loc[:5]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny\nage\nrace\nprior\ngender\njuvenilecrime\ncurrentcharge\ntrain_test\n\n\n\n\n0\n0\n0\n1\n1\n1\n0\ntrain\n\n\n1\n0\n1\n1\n1\n1\n0\ntrain\n\n\n1\n0\n1\n0\n1\n0\n0\ntrain\n\n\n0\n0\n1\n0\n1\n0\n0\ntrain\n\n\n0\n0\n0\n0\n1\n1\n0\ntrain\n\n\n0\n0\n0\n1\n1\n1\n1\ntrain\n\n\n\n\n\n\ny: 2-year recidivism (the binary outcome, 1=event and 0=non-event).\nage, race, prior, gender, juvenilecrime, currentcharge: binary predictors.\ntrain_test: training/explanation set membership indicator (\"train\" for training and \"test\" for explanation). Not to include in models.\n\n\n\n2.1.2 Prepare training and explanation sets\n\nWhen there is sufficient data, users can split the full dataset into a training set to train optimal and nearly optimal models, and an explanation set to compute ShapleyVIC values.\nOtherwise, users may use the full dataset to train models and compute ShapleyVIC values.\n\n\n\n\n\n\n\nGeneral suggestions on the size of explanation set\n\n\n\n\nLarger number of variables generally requires larger explanation set for stable results.\nIncrease in the size of explanation set and/or number of variables increases time required to compute ShapleyVIC values.\nUse of >3500 samples in explanation set leads to long run time and is generally not recommended.\n\n\n\nIn the experiment, we used 10% of the full dataset as explanation set:\n\nRandom splitUse the train_test indicator\n\n\n\n# Drop the column 'train_test' that indicates set membership in example data:\ncompas = compas.drop(columns=['train_test'])\n# Generate row indices for training and explanation sets:\nfrom sklearn.model_selection import train_test_split\ni_train, i_expl = train_test_split(list(range(compas.shape[0])), \n    test_size=int(0.1 * compas.shape[0]), random_state=0)\n\ndat_train = compas.iloc[i_train, :]\ndat_train.reset_index(drop=True, inplace=True)\n\ndat_expl = compas.iloc[i_expl, :]\ndat_expl.reset_index(drop=True, inplace=True)\n\n\n\n\ndat_train = compas.loc[compas['train_test']=='train']\n# Drop the indicator column after using it to split data:\ndat_train = dat_train.drop(columns=['train_test'])\ndat_train.reset_index(drop=True, inplace=True)\n\ndat_expl = compas.loc[compas['train_test']=='test']\ndat_expl = dat_expl.drop(columns=['train_test'])\ndat_expl.reset_index(drop=True, inplace=True)\n\n\n\n\n\n\n2.1.3 Train optimal model\n\nSpecify training data to initialize the model object and train the optimal model.\nx, y: predictors (as a data frame) and outcome from the training set.\noutcome_type: type of the outcome (currently only supports binary outcomes).\nx_names_cat: names of categorical predictors. Optional for binary predictors encoded as 0/1.\noutput_dir: the directory to save key outputs to. Will be used as input in the subsequent R workflow.\nsave_data: whether to save x and y to output_dir (default is to save). If not, x and y must be supplied separately in subsequent R analysis.\n\nSee Chapter 2 for an example with save_data=False.\n\n\n\n# Specify the name of outcome, which is 'y' in this example:\ny_name = 'y'\nfrom ShapleyVIC import model\nmodel_object = model.models(\n    x=dat_train.drop(columns=[y_name]), y=dat_train[y_name], \n    outcome_type=\"binary\", \n    x_names_cat=['age','race','prior','gender','juvenilecrime','currentcharge'],\n    output_dir=\"compas_output\"\n)\n# To display the optimal logistic regression trained:\nmodel_object.model_optim.summary().tables[1]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP\n[0.025\n0.975]\n\n\n\n\nconst\n0.4455\n0.107\n4.160\n0.000\n0.236\n0.655\n\n\nage\n1.5001\n0.187\n8.011\n0.000\n1.133\n1.867\n\n\nrace\n0.4164\n0.053\n7.858\n0.000\n0.313\n0.520\n\n\nprior\n-0.8543\n0.061\n-13.984\n0.000\n-0.974\n-0.735\n\n\ngender\n0.3835\n0.068\n5.651\n0.000\n0.251\n0.517\n\n\njuvenilecrime\n-0.8646\n0.084\n-10.238\n0.000\n-1.030\n-0.699\n\n\ncurrentcharge\n-0.2544\n0.056\n-4.562\n0.000\n-0.364\n-0.145\n\n\n\n\n\n2.1.4 Generate nearly optimal models\nNearly optimal logistic regression models are defined as models with logistic loss less than \\((1+\\varepsilon)\\) times the minimum loss (i.e., logistic loss of the optimal model). Default value for \\(\\varepsilon\\) is 5%.\n\nu1 and u2 are key hyper-parameters for generating nearly optimal models, which control the sampling range of initial models to fully explore the model space.\nUse the following command to generate a set of reasonable values for u1 and u2 (using m=200 initial models), such that approximately 70%-80% of initial models are eligible:\n\n\nu1, u2 = model_object.init_hyper_params(m=200)\n(u1, u2)\n\n(0.5, 80.3125)\n\nUse the following command to generate a final set of nearly optimal models (e.g., n_final=250) from 500 initial samples (m=500):\n\n\nmodel_object.draw_models(u1=u1, u2=u2, m=500, n_final=250, random_state=1234)\nmodel_object.models_plot\n\n\n\n\n\n\n\n\nmodel_object.models_near_optim.iloc[:5]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst\nage_1\nrace_1\nprior_1\ngender_1\njuvenilecrime_1\ncurrentcharge_1\nperf_metric\n\n\n\n\n-0.2307\n3.1195\n0.5047\n-1.1409\n0.2644\n-0.1170\n0.2664\n1.0280\n\n\n0.5503\n0.7759\n0.8971\n-1.1164\n-0.3083\n-0.6398\n-0.1481\n1.0285\n\n\n0.1068\n0.8697\n-0.0176\n-0.6963\n0.6987\n-0.5041\n-0.1812\n1.0187\n\n\n0.9715\n0.8669\n-0.1101\n-1.0772\n0.6450\n-1.3590\n-0.3310\n1.0212\n\n\n-1.0476\n2.0026\n0.6911\n-0.3203\n1.4661\n-0.6633\n-0.0397\n1.0438\n\n\n0.4006\n1.6629\n0.1719\n-0.5450\n0.3218\n-0.9498\n0.6260\n1.0445\n\n\n\n\n\n\n\n2.1.5 Assess variable importance\nThis step assesses variable importance for each nearly optimal model generated in the previous step using the SAGE method, and write the results to the output folder for further processing in the subsequent R workflow. Parallel processing is used to reduce run time.\n\nmodel_object: the model object created above.\nx_expl, y_expl: predictors (as a data frame) and outcome from the explanation set.\nn_cores: number of CPU cores to use in parallel processing.\n\nFor a computer with n cores, do not use more than n-1 cores.\n\nthreshold: threshold parameter used in SAGE algorithm for convergence criterion. A reasonable value is 0.05 (default).\n\nSmaller threshold value may improve accuracy of uncertainty measure but notably increases run time.\n\n\n\nfrom ShapleyVIC import compute\nm_svic = compute.compute_shapley_vic(\n    model_obj=model_object, \n    x_expl=dat_expl.drop(columns=[y_name]), y_expl=dat_expl[y_name], \n    n_cores=7, # running on a MacBook Air with 8 cores\n    threshold=0.05\n)\n\n\n\n\n\n\n\nNote\n\n\n\n\nUse built-in software (e.g., Activity Monitor/Task Manager) to monitor CPU and Memory usage. Avoid taking up 100% CPU, which can slow down computation.\nThis step can be time consuming with larger number of variables and/or larger explanation data.\nFor users’ reference, the command above took approximately 11 minutes on a 2022 MacBook Air (Apple M2 chip with 8‑core CPU, 8-core GPU; 16GB unified memory; 256GB SSD storage)."
  },
  {
    "objectID": "shapleyvic.html#r-shapleyvic-summary-and-visualizations",
    "href": "shapleyvic.html#r-shapleyvic-summary-and-visualizations",
    "title": "2  ShapleyVIC for Variable Importance Assessment",
    "section": "2.2 [R] ShapleyVIC summary and visualizations",
    "text": "2.2 [R] ShapleyVIC summary and visualizations\nThis part of the ShapleyVIC workflow is implemented in R.\nThis part of the workflow works on output from Python (all saved in output_dir), pooling information across models to compute (and visualize) overall variable importance and derive ensemble variable rankings.\n\n2.2.1 Compute overall importance\nAs detailed in the paper, raw Shapley-based variable importance needs to be adjusted based on variable colinearity to derive final ShapleyVIC values.\n\noutput_dir: output folder generated from the Python workflow.\noutcome_type: type of outcome, as specified in the Python workflow.\nx and y: training data specified in the Python workflow, required if save_data=False was specified when setting up model.models(...) in Python.\n\nSee Chapter 2 for an example with save_data=False.\n\nx_names_cat: names of categorical variables, as specified in the Python workflow. Used when assessing variable colinearity from the training set. Optional for binary variables coded as 0/1.\nx_names_display: variable names to use in summary statistics and visualizations. If not provided, column names in the training set will be used.\n\n\nlibrary(ShapleyVIC)\nmodel_object <- compile_shapley_vic(\n  output_dir = \"compas_output\", outcome_type = \"binary\", \n  x_names_cat = c('age','race','prior','gender','juvenilecrime','currentcharge'),\n  x_names = c(\"Age\", \"Race\", \"Prior criminal history\", \"Gender\", \n              \"Juvenile criminal history\", \"Current charge\")\n)\n\n\n\n2.2.2 Visualize overall variable importance\nEach ShapleyVIC value (shapley_vic_val) is reported with a standard deviation (sage_sd). We pool information across models to compute overall variable importance and uncertainty interval, visualized using bar plot. The relationship between variable importance and model performance is visualized using violin plot.\n\nFor clarity, in the bar plot variables with significant overall importance are indicated by blue color and “*” next to variable names.\n\n\nmodel_plots <- plot(model_object)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nPlots above reproduce key findings reported in the paper: race had non-significant overall importance, and prior criminal history and juvenile criminal history had higher overall importance than other variables.\nOverall importance of age now becomes non-significant, showing that data sparsity (only 20 [2.8%] of 721 subjects had age=1 in explanation data) leads to less stable results.\n\n\n\nThe bar plot can be further edited using ggplot functions, e.g., edit text font size using theme() or add plot title using labs():\n\nlibrary(ggplot2)\nmodel_plots$bar + theme(text = element_text(size = 14)) + labs(title = \"Bar plot\")\n\nTo apply similar formatting to the violin plot, use the following function:\n\nlibrary(ggplot2)\nplot_violin(x = model_object, title = \"Violin plot\", \n            plot_theme = theme(text = element_text(size = 14)))\n\n\n\n2.2.3 Ensemble variable ranking\nShapleyVIC values can also be used to rank variables by their importance to each model. The bar plot of ranks may help identify models with increased reliance on specific variable of interest for further investigation.\n\nval_ranks <- rank_variables(model_object)\nhead(val_ranks, 6)\n\n  model_id                  Variable rank\n1        0                       Age    3\n2        0                      Race    3\n3        0    Prior criminal history    1\n4        0                    Gender    3\n5        0 Juvenile criminal history    2\n6        0            Current charge    6\n\nlibrary(ggplot2)\nggplot(val_ranks, aes(x = rank, group = Variable)) + \n  geom_bar() + \n  facet_wrap(~ Variable, nrow = 2) + \n  theme_bw() + \n  labs(x = \"Ranking\", y = \"\", \n       title = \"ShapleyVIC: Variable ranking among 250 models\")\n\n\n\n\nThe ensemble ranking averages the ranks across models, and can be used to guide downstream model building, e.g., using AutoScore. See the next chapter for detailed demonstration.\n\nrank_variables(model_object, summarise = TRUE)\n\n                   Variable mean_rank\n1 Juvenile criminal history     1.224\n2    Prior criminal history     1.536\n\n# To return variable ranking as named vector for convenient integration with \n# AutoScore:\nrank_variables(model_object, summarise = TRUE, as_vector = TRUE)\n\nJuvenile criminal history    Prior criminal history \n                    1.224                     1.536"
  },
  {
    "objectID": "autoscore_shapleyvic.html#r-prepare-data",
    "href": "autoscore_shapleyvic.html#r-prepare-data",
    "title": "3  AutoScore-ShapleyVIC for Interpretable Risk Score Development",
    "section": "3.1 [R] Prepare data",
    "text": "3.1 [R] Prepare data\nThis part of the workflow is implemented in R.\n\n3.1.1 Load R packages and data\n\nif (!require(AutoScore, quietly = TRUE)) install.packages(\"AutoScore\")\nlibrary(AutoScore)\nlibrary(tidyverse) # For convenient data manipulation and visualization\n\n# Read the final clean data with 41 candidate variables and the binary outcome \n# (`label`):\ndat <- readRDS(\"dat_readmit_or_death.RDS\")\n\n\n\n\n\n\n3.1.2 Prepare training, validation and test datasets\n\nUse the split_data() function of the AutoScore package to split data into training (70%), validation (10%) and test (20%) sets for risk score development.\nPerform median imputation for vital signs and lab tests based on training set.\n\n\nset.seed(1234)\nOut_split <- split_data(data = dat, ratio = c(7, 1, 2))\n# Median imputation for vital signs and lab tests based on training set:\ntrain_lab_test <- Out_split$train_set %>% select(Pulse:SODIUM)\ntrain_lab_test_median <- apply(train_lab_test, 2, function(x) median(x, na.rm = TRUE))\nOut_split <- lapply(Out_split, function(dat) {\n  for (nm in names(train_lab_test)) {\n    dat[, nm] <- ifelse(is.na(dat[, nm]), train_lab_test_median[nm], dat[, nm])\n  }\n  dat\n})\n\ntrain_set <- Out_split$train_set\nvalidation_set <- Out_split$validation_set\ntest_set <- Out_split$test_set\n\n\nPrepare output_dir for ShapleyVIC, using train_set as training set and the first 3500 observations in validation_set as the explanation data.\n\n\noutput_dir <- \"score_output\"\nif (!dir.exists(output_dir)) dir.create(output_dir)\nwrite.csv(train_set, file = file.path(output_dir, \"train_set.csv\"), \n          row.names = FALSE)\nwrite.csv(validation_set[1:3500, ], \n          file = file.path(output_dir, \"validation_set.csv\"), \n          row.names = FALSE)"
  },
  {
    "objectID": "autoscore_shapleyvic.html#python-compute-shapleyvic-values",
    "href": "autoscore_shapleyvic.html#python-compute-shapleyvic-values",
    "title": "3  AutoScore-ShapleyVIC for Interpretable Risk Score Development",
    "section": "3.2 [Python] Compute ShapleyVIC values",
    "text": "3.2 [Python] Compute ShapleyVIC values\nThis part of the workflow is implemented in Python.\n\nLoad data and set up input information.\n\nData used in this analysis is sensitive, therefore we do not save training data to the output folder to avoid any potential data security issue.\n\nimport os\nimport pandas as pd\noutput_dir = \"score_output\"\ndat_train = pd.read_csv(os.path.join(output_dir, 'train_set.csv'))\ndat_expl = pd.read_csv(os.path.join(output_dir, 'validation_set.csv'))\n\ny_name = 'label'\nx_names_cat = ['Gender','Race','Triage_Class_Code','DayofWeek','MI','CHF','PVD',\n    'Stroke','Dementia','Pulmonary','Rheumatic','PUD','LiverMild','Diabetes',\n    'DMcx','Paralysis','Renal','Cancer','LiverSevere','Mets','admit_cat',\n    'resuscitation','VENTILATION']\nfrom ShapleyVIC import model\nmodel_object = model.models(\n    x=dat_train.drop(columns=[y_name]), y=dat_train[y_name], \n    x_names_cat=x_names_cat, outcome_type=\"binary\", output_dir=output_dir,\n    save_data=False\n)\n\n\nDraw 350 nearly optimal models.\n\n\nmodel_object.draw_models(u1=0.2, u2=300, m=800, n_final=350)\n\n\nCompute ShapleyVIC values.\n\n\nfrom ShapleyVIC import compute\nm_svic = compute.compute_shapley_vic(\n    model_obj=model_object, \n    x_expl=dat_expl.drop(columns=[y_name]), y_expl=dat_expl[y_name], \n    n_cores=10, # running on a PC with 40 logical processors\n    threshold=0.025\n)"
  },
  {
    "objectID": "autoscore_shapleyvic.html#r-develop-risk-score",
    "href": "autoscore_shapleyvic.html#r-develop-risk-score",
    "title": "3  AutoScore-ShapleyVIC for Interpretable Risk Score Development",
    "section": "3.3 [R] Develop risk score",
    "text": "3.3 [R] Develop risk score\nThis part of the workflow is implemented in R.\n\n3.3.1 Rank variables using ShapleyVIC\n\nCompile ShapleyVIC output.\nSince data was not saved in the Python workflow, we explicitly specify it in the R analysis.\nExplicitly specify names of categorical variables, identical to those specified in the Python workflow.\n\n\noutput_dir <- \"score_output\"\nx_names_display <- c(\n  \"Age\", \"Gender\", \"Race\", \"ED LOS\", \"ED triage\", \n  \"ED boarding time\", \"Consultation waiting time\", \"No. ED visit\", \n  \"Day of week\", \"Inpatient LOS\", \"Ventilation\", \"Resuscitation\", \n  \"No. surgery\", \"No. ICU stay\", \n  \"No. HD stay\", \"Pulse\", \"Respiration\", \"SpO2\", \n  \"DBP\", \"SBP\", \"Bicarbonate\", \"Creatinine\", \n  \"Potasium\", \"Sodium\", \"MI\", \"CHF\", \"PVD\", \"Stroke\", \n  \"Dementia\", \"Pulmonary\", \"Rheumatic\", \"PUD\", \"Mild liver disease\", \n  \"Diabetes\", \"Diabetes with complications\", \"Paralysis\", \"Renal\", \"Cancer\", \n  \"Severe liver disease\", \"Metastatic cancer\", \"Admission type\"\n)\ny_name <- \"label\"\nx_names <- setdiff(names(train_set), y_name)\nlibrary(ShapleyVIC)\nmodel_object <- compile_shapley_vic(\n  output_dir = output_dir, outcome_type = \"binary\", \n  x = train_set[, x_names], y = train_set$label,\n  x_names_cat = c(\n    'Gender','Race','Triage_Class_Code','DayofWeek','MI','CHF','PVD',\n    'Stroke','Dementia','Pulmonary','Rheumatic','PUD','LiverMild','Diabetes',\n    'DMcx','Paralysis','Renal','Cancer','LiverSevere','Mets','admit_cat',\n    'resuscitation','VENTILATION'\n  ),\n  x_names = x_names_display\n)\n\n\n\n\n\nVisualize ShapleyVIC values for overall variable importance.\n\n\nmodel_plots <- plot(model_object)\n\nThe following variables are excluded due to zero importance in all models analysed:\n\n\n     Ventilation \n\n\n\n\n\n\n\n\n\nDerive ShapleyVIC-based ensemble variable ranking.\n\n\nranking <- rank_variables(model_object, summarise = TRUE, as_vector = TRUE)\nranking\n\n               No. ED visit           Metastatic cancer \n                   1.000000                    2.182857 \n                        Age                      Sodium \n                   3.057143                    6.931429 \n                      Renal                   ED triage \n                   6.945714                    7.428571 \n       Severe liver disease                         CHF \n                   9.071429                   10.365714 \n                 Creatinine                         PVD \n                  10.642857                   12.074286 \n                        SBP                      Cancer \n                  14.120000                   14.597143 \n              Inpatient LOS                      ED LOS \n                  16.105714                   16.908571 \n         Mild liver disease                   Pulmonary \n                  17.785714                   18.040000 \n                   Dementia Diabetes with complications \n                  19.697143                   20.291429 \n                      Pulse                          MI \n                  20.691429                   21.214286 \n                     Stroke \n                  24.185714 \n\n\n\n\n3.3.2 Develop risk score using AutoScore workflow\n\nModify variable names in training, validation and test sets for publication-ready figures and printed output.\n\n\n# Current raw variable names:\nnames(train_set)\n\n [1] \"label\"                     \"Age\"                      \n [3] \"Gender\"                    \"Race\"                     \n [5] \"ED_LOS\"                    \"Triage_Class_Code\"        \n [7] \"EDBoardingTime\"            \"ConsultationWaitingTime\"  \n [9] \"n_ed_6mth\"                 \"DayofWeek\"                \n[11] \"LOS_inp\"                   \"VENTILATION\"              \n[13] \"resuscitation\"             \"Total_Num_Surgery_last1yr\"\n[15] \"Total_icu_count_last1yr\"   \"Total_hd_count_last1yr\"   \n[17] \"Pulse\"                     \"Respiration\"              \n[19] \"SPO2\"                      \"BP_Diastolic\"             \n[21] \"BP_Systolic\"               \"BICARBONATE\"              \n[23] \"CREATININE\"                \"POTASSIUM\"                \n[25] \"SODIUM\"                    \"MI\"                       \n[27] \"CHF\"                       \"PVD\"                      \n[29] \"Stroke\"                    \"Dementia\"                 \n[31] \"Pulmonary\"                 \"Rheumatic\"                \n[33] \"PUD\"                       \"LiverMild\"                \n[35] \"Diabetes\"                  \"DMcx\"                     \n[37] \"Paralysis\"                 \"Renal\"                    \n[39] \"Cancer\"                    \"LiverSevere\"              \n[41] \"Mets\"                      \"admit_cat\"                \n\n# Modified variable names:\nnames(train_set)[-1] <- x_names_display\nnames(train_set)\n\n [1] \"label\"                       \"Age\"                        \n [3] \"Gender\"                      \"Race\"                       \n [5] \"ED LOS\"                      \"ED triage\"                  \n [7] \"ED boarding time\"            \"Consultation waiting time\"  \n [9] \"No. ED visit\"                \"Day of week\"                \n[11] \"Inpatient LOS\"               \"Ventilation\"                \n[13] \"Resuscitation\"               \"No. surgery\"                \n[15] \"No. ICU stay\"                \"No. HD stay\"                \n[17] \"Pulse\"                       \"Respiration\"                \n[19] \"SpO2\"                        \"DBP\"                        \n[21] \"SBP\"                         \"Bicarbonate\"                \n[23] \"Creatinine\"                  \"Potasium\"                   \n[25] \"Sodium\"                      \"MI\"                         \n[27] \"CHF\"                         \"PVD\"                        \n[29] \"Stroke\"                      \"Dementia\"                   \n[31] \"Pulmonary\"                   \"Rheumatic\"                  \n[33] \"PUD\"                         \"Mild liver disease\"         \n[35] \"Diabetes\"                    \"Diabetes with complications\"\n[37] \"Paralysis\"                   \"Renal\"                      \n[39] \"Cancer\"                      \"Severe liver disease\"       \n[41] \"Metastatic cancer\"           \"Admission type\"             \n\nnames(validation_set)[-1] <- x_names_display\nnames(test_set)[-1] <- x_names_display\n\n\nBased on the ensemble variable ranking, apply AutoScore STEP(ii) to select the best model with parsimony plot.\n\n\nAUC <- AutoScore_parsimony(\n  train_set = train_set, validation_set = validation_set, \n  rank = ranking, max_score = 100, n_min = 1, n_max = length(ranking)\n)\n\nSelect 1 Variable(s):  Area under the curve: 0.6811\nSelect 2 Variable(s):  Area under the curve: 0.706\nSelect 3 Variable(s):  Area under the curve: 0.7406\nSelect 4 Variable(s):  Area under the curve: 0.7467\nSelect 5 Variable(s):  Area under the curve: 0.7555\nSelect 6 Variable(s):  Area under the curve: 0.7589\nSelect 7 Variable(s):  Area under the curve: 0.7595\nSelect 8 Variable(s):  Area under the curve: 0.7605\nSelect 9 Variable(s):  Area under the curve: 0.7624\nSelect 10 Variable(s):  Area under the curve: 0.7637\nSelect 11 Variable(s):  Area under the curve: 0.765\nSelect 12 Variable(s):  Area under the curve: 0.7674\nSelect 13 Variable(s):  Area under the curve: 0.7696\nSelect 14 Variable(s):  Area under the curve: 0.7708\nSelect 15 Variable(s):  Area under the curve: 0.7708\nSelect 16 Variable(s):  Area under the curve: 0.7713\nSelect 17 Variable(s):  Area under the curve: 0.7713\nSelect 18 Variable(s):  Area under the curve: 0.7712\nSelect 19 Variable(s):  Area under the curve: 0.7713\nSelect 20 Variable(s):  Area under the curve: 0.7715\nSelect 21 Variable(s):  Area under the curve: 0.7718\n\n\n\n\n\n\nThis parsimony is somewhat smoother than that from random forest-based variable ranking used in AutoScore.\nA feasible choice is to select the top 6 variables, as adding additional variables does not substantially improve model performance.\nApply AutoScore STEP(iii) to build initial scores from the top 6 variables.\n\n\ncut_vec <- AutoScore_weighting(\n  train_set = train_set, validation_set = validation_set, \n  final_variables = names(ranking)[1:6], max_score = 100\n)\n\n****Included Variables: \n      variable_name\n1      No. ED visit\n2 Metastatic cancer\n3               Age\n4            Sodium\n5             Renal\n6         ED triage\n****Initial Scores: \n\n\n=================  =========  =====\nvariable           interval   point\n=================  =========  =====\nNo. ED visit       <1           0  \n                   [1,3)       14  \n                   >=3         32  \n                                   \nMetastatic cancer  0            0  \n                   1           22  \n                                   \nAge                <28          0  \n                   [28,46)      5  \n                   [46,78)     11  \n                   [78,87)     14  \n                   >=87        19  \n                                   \nSodium             <126        11  \n                   [126,132)    8  \n                   [132,138)    3  \n                   [138,141)    0  \n                   >=141        3  \n                                   \nRenal              0            0  \n                   1            8  \n                                   \nED triage          P1           8  \n                   P2           5  \n                   P3 and P4    0  \n=================  =========  =====\n\n\n\n\n\n***Performance (based on validation set):\nAUC:  0.7589   95% CI: 0.7525-0.7654 (DeLong)\nBest score threshold: >= 27 \nOther performance indicators based on this score threshold: \nSensitivity: 0.7546\nSpecificity: 0.6338\nPPV:         0.2888\nNPV:         0.9291\n***The cutoffs of each variable generated by the AutoScore are saved in cut_vec. You can decide whether to revise or fine-tune them \n\n\n\nUsers can apply additional AutoScore STEPs for subsequent model fine-tuning and evaluation."
  },
  {
    "objectID": "autoscore_shapleyvic_reprod.html#r-prepare-data",
    "href": "autoscore_shapleyvic_reprod.html#r-prepare-data",
    "title": "4  AutoScore-ShapleyVIC Reproducible Example",
    "section": "4.1 [R] Prepare data",
    "text": "4.1 [R] Prepare data\nThis part of the workflow is implemented in R.\n\n4.1.1 Load data\n\nLoad sample_data from the AutoScore package.\nAs required by AutoScore, change the name of outcome variable to label.\nRead AutoScore Guidebook for detailed data requirement.\n\n\nlibrary(AutoScore)\ndata(\"sample_data\")\nnames(sample_data)[names(sample_data) == \"Mortality_inpatient\"] <- \"label\"\ncheck_data(sample_data)\n\nData type check passed. \n\n\nNo NA in data \n\ndim(sample_data)\n\n[1] 20000    22\n\n\n\nAs required by ShapleyVIC, code the binary outcome as 0/1.\nAll variables are continuous.\n\n\nsample_data$label <- as.numeric(sample_data$label == \"TRUE\")\nhead(sample_data)\n\n  Vital_A Vital_B Vital_C Vital_D Vital_E Vital_F Vital_G Lab_A Lab_B Lab_C\n1      87     143      78     101      13    35.7      99   160  13.0    23\n2      43     133      64      83      20    36.1      95   116  15.3    24\n3      80     115      48      72      23    37.4      99   133   8.0    27\n4     106     121      68      84      16    37.6      99   206  12.1    25\n5      86     135      70      83      24    37.2      96   100  18.1    26\n6      69     123      72      88      16    36.5      95   204  19.9    20\n  Lab_D Lab_E Lab_F Lab_G Lab_H Lab_I Lab_J Lab_K Lab_L Lab_M Age label\n1   0.0   105    34    12   0.8    98   4.4     0   136    16  66     0\n2   0.8   108    36    12   0.6   322   4.3    55   141    17  79     0\n3   1.3   111    30    11   2.9     0   4.4    40   142     0  86     0\n4   0.0   102    39    14   3.0   214   4.4     0   134     6  69     0\n5   2.3    96    36    13   2.7   326   3.8    20   134    26  65     0\n6   2.5   101    31    10   0.8   103   4.2    38   138    14  68     0\n\n\n\n\n4.1.2 Prepare training, validation, and test datasets\n\nGiven large sample size (n=20000), split the data into training (70%), validation (10%) and test (20%) sets for risk score development.\n\n\nset.seed(4)\nout_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))\ntrain_set <- out_split$train_set\ndim(train_set)\n\n[1] 14000    22\n\nvalidation_set <- out_split$validation_set\ndim(validation_set)\n\n[1] 2000   22\n\ntest_set <- out_split$test_set\ndim(test_set)\n\n[1] 4000   22\n\n\n\nPrepare output_dir for ShapleyVIC, using train_set as training set and validation_set as the explanation data.\n\n\noutput_dir <- \"mort_output\"\nif (!dir.exists(output_dir)) dir.create(output_dir)\nwrite.csv(train_set, file = file.path(output_dir, \"train_set.csv\"), \n          row.names = FALSE)\nwrite.csv(validation_set, \n          file = file.path(output_dir, \"validation_set.csv\"), \n          row.names = FALSE)"
  },
  {
    "objectID": "autoscore_shapleyvic_reprod.html#python-compute-shapleyvic-values",
    "href": "autoscore_shapleyvic_reprod.html#python-compute-shapleyvic-values",
    "title": "4  AutoScore-ShapleyVIC Reproducible Example",
    "section": "4.2 [Python] Compute ShapleyVIC values",
    "text": "4.2 [Python] Compute ShapleyVIC values\nThis part of the workflow is implemented in Python.\n\nLoad data and set up input information.\n\n\nimport os\nimport pandas as pd\noutput_dir = \"mort_output\"\ndat_train = pd.read_csv(os.path.join(output_dir, 'train_set.csv'))\ndat_expl = pd.read_csv(os.path.join(output_dir, 'validation_set.csv'))\n\ny_name = 'label'\nfrom ShapleyVIC import model\nmodel_object = model.models(\n    x=dat_train.drop(columns=[y_name]), y=dat_train[y_name], \n    # No need to specify x_names_cat because all variables are continuous\n    outcome_type=\"binary\", output_dir=output_dir\n)\n\n\nSet values for hyper-parameters u1 and u2.\n\n\nu1, u2 = model_object.init_hyper_params(m=200)\n(u1, u2)\n\n(0.5, 15.625)\n\nDraw 250 nearly optimal models from 500 initial samples.\n\n\nmodel_object.draw_models(u1=u1, u2=u2, m=500, n_final=250, random_state=1234)\nmodel_object.models_plot\n\n\n\n\n\n\n\n\nCompute ShapleyVIC values.\n\n\nfrom ShapleyVIC import compute\nm_svic = compute.compute_shapley_vic(\n    model_obj=model_object, \n    x_expl=dat_expl.drop(columns=[y_name]), y_expl=dat_expl[y_name], \n    n_cores=25, # running on a PC with 40 logical processors\n    threshold=0.05\n)\n\n\n\n\n\n\n\nNote\n\n\n\n\nFor users’ reference, the command above took approximately 17 hours on a PC (Windows 10 Education; Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz 2.19GHz (2 processors); 128GB RAM)."
  },
  {
    "objectID": "autoscore_shapleyvic_reprod.html#r-develop-risk-score",
    "href": "autoscore_shapleyvic_reprod.html#r-develop-risk-score",
    "title": "4  AutoScore-ShapleyVIC Reproducible Example",
    "section": "4.3 [R] Develop risk score",
    "text": "4.3 [R] Develop risk score\nThis part of the workflow is implemented in R.\n\n4.3.1 Rank variables using ShapleyVIC\n\nCompile ShapleyVIC output.\n\n\nlibrary(ShapleyVIC)\nmodel_object <- compile_shapley_vic(\n  output_dir = output_dir, outcome_type = \"binary\"\n)\n\n\nVisualize ShapleyVIC values for overall variable importance.\n\n\nmodel_plots <- plot(model_object)\n\n\n\n\n\n\n\n\nDerive ShapleyVIC-based ensemble variable ranking.\n\n\nranking <- rank_variables(model_object, summarise = TRUE, as_vector = TRUE)\nranking\n\n    Age   Lab_H Vital_E   Lab_K Vital_A   Lab_B   Lab_M Vital_C   Lab_C   Lab_D \n  1.000   2.116   3.028   4.104   5.588   5.744   8.904   8.940   9.084   9.704 \n  Lab_L   Lab_E Vital_D   Lab_G Vital_B   Lab_J   Lab_F \n 10.120  10.268  11.628  12.952  13.220  13.328  16.052 \n\n\n\n\n4.3.2 Develop risk score using AutoScore workflow\n\nBased on the ensemble variable ranking, apply AutoScore STEP(ii) to select the best model with parsimony plot.\n\n\nAUC <- AutoScore_parsimony(\n  train_set = train_set, validation_set = validation_set, \n  rank = ranking, max_score = 100, n_min = 1, n_max = length(ranking)\n)\n\nSelect 1 Variable(s):  Area under the curve: 0.6649\nSelect 2 Variable(s):  Area under the curve: 0.7466\nSelect 3 Variable(s):  Area under the curve: 0.7881\nSelect 4 Variable(s):  Area under the curve: 0.8009\nSelect 5 Variable(s):  Area under the curve: 0.8137\nSelect 6 Variable(s):  Area under the curve: 0.8268\nSelect 7 Variable(s):  Area under the curve: 0.8232\nSelect 8 Variable(s):  Area under the curve: 0.8234\nSelect 9 Variable(s):  Area under the curve: 0.8215\nSelect 10 Variable(s):  Area under the curve: 0.8286\nSelect 11 Variable(s):  Area under the curve: 0.8267\nSelect 12 Variable(s):  Area under the curve: 0.8294\nSelect 13 Variable(s):  Area under the curve: 0.8281\nSelect 14 Variable(s):  Area under the curve: 0.8246\nSelect 15 Variable(s):  Area under the curve: 0.8293\nSelect 16 Variable(s):  Area under the curve: 0.8275\nSelect 17 Variable(s):  Area under the curve: 0.8278\n\n\n\n\n\n\nThis parsimony is somewhat smoother than that from random forest-based variable ranking used in AutoScore.\nA feasible choice is to select the top 6 variables, as adding additional variables does not substantially improve model performance.\nApply AutoScore STEP(iii) to build initial scores from the top 6 variables.\n\n\ncut_vec <- AutoScore_weighting(\n  train_set = train_set, validation_set = validation_set, \n  final_variables = names(ranking)[1:6], max_score = 100\n)\n\n****Included Variables: \n  variable_name\n1           Age\n2         Lab_H\n3       Vital_E\n4         Lab_K\n5       Vital_A\n6         Lab_B\n****Initial Scores: \n\n\n========  ==========  =====\nvariable  interval    point\n========  ==========  =====\nAge       <35           0  \n          [35,49)       7  \n          [49,76)      17  \n          [76,89)      23  \n          >=89         27  \n                           \nLab_H     <0.2          0  \n          [0.2,1.1)     4  \n          [1.1,3.1)     9  \n          [3.1,4)      15  \n          >=4          18  \n                           \nVital_E   <12           0  \n          [12,15)       2  \n          [15,22)       7  \n          [22,25)      12  \n          >=25         15  \n                           \nLab_K     <8            0  \n          [8,42)        6  \n          [42,58)      11  \n          >=58         14  \n                           \nVital_A   <60           0  \n          [60,73)       1  \n          [73,98)       6  \n          [98,111)     10  \n          >=111        13  \n                           \nLab_B     <8.5          0  \n          [8.5,11.2)    4  \n          [11.2,17)     7  \n          [17,19.8)    10  \n          >=19.8       12  \n========  ==========  =====\n\n\n\n\n\n***Performance (based on validation set):\nAUC:  0.8268   95% CI: 0.7953-0.8583 (DeLong)\nBest score threshold: >= 57 \nOther performance indicators based on this score threshold: \nSensitivity: 0.8065\nSpecificity: 0.6775\nPPV:         0.1736\nNPV:         0.9766\n***The cutoffs of each variable generated by the AutoScore are saved in cut_vec. You can decide whether to revise or fine-tune them \n\n\n\nUsers can apply additional AutoScore STEPs for subsequent model fine-tuning and evaluation."
  }
]